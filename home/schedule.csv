Time,Program
09:00-09:05,Opening Remarks
09:05-09:40,"Keynote Speech <b>Peter Clark: What do our Machines Believe?</b><br><i>Do language models form anything like a 'mental model' when reasoning? And do they have coherent 'beliefs' about the world? Probing an LM, we find the LLM's world views are only partially coherent, and often contain blatent inconsistencies. Taking this further, I'll describe how we can extract 'belief graphs' from LMs and repair the inconsistencies they uncover. More generally, I'll promote a two-layered architecture for future systems, consisting of the LM plus a symbolic representation of (parts of) the model's belief state, supporting systematic reasoning, interaction, addition of external knowledge, and more rational behavior by our future LM companions.</i>"
09:40-10:15,"Keynote Speech <b>Luke Zettlemoyer: Chameleon: Universal Mixed-modal Modeling by Tokenizing Everything</b><br><i>Existing multimodal models typically have custom architectures that are designed for specific modalities (image->text, text->image, text only, etc). In this talk, I will present our recent work on Chameleon, a family of early-fusion token-based mixed-modal models capable of understanding and generating images and text in any arbitrary sequence. The key idea is to tokenize images into a discrete space, wherever they appear within multimodal documents, and then model the resulting sequences of mixed-modal tokens with a single unified transformer. This approach allows us to trivial lift all of the advanced modeling techniques originally developed for text-only models to the multimodal setting, including multi-task alignment and retrieval augmentation, as I will show. It also performs well overall, demonstrating broad and general capabilities, including state-of-the-art performance in image captioning tasks, outperforms Llama-2 in text-only tasks while being competitive with models such as Mixtral 8x7B and Gemini-Pro, and performs non-trivial image generation, all in a single model.</i>"
10:15-10:50,Keynote Speech Tatsu Hashimoto 
10:50-11:05,Coffee Break
11:05-12:25,Oral Presentation: Modeling Uncertainty and Using Post-fusion as Fallback Improves Retrieval Augmented Generation with LLMs 
,Oral Presentation: AcKnowledge: Acquired Knowledge Representation by Small Language Model Without Pre-training 
,Oral Presentation: Unified Hallucination Detection for Multimodal Large Language Models
,Oral Presentation: Is Table Retrieval a Solved Problem? Join-Aware Multi-Table Retrieval 
,Oral Presentation: Measuring the Inconsistency of Large Language Models in Preferential Ranking 
,"Oral Presentation: Merging Facts, Crafting Fallacies: Evaluating the Contradictory Nature of Aggregated Factual Claims in Long-Form Generations"
12:25-12:30,Best Paper and Outstanding Paper Announcement
12:40-13:30,Lunch Break
13:30-14:05,Keynote Speech Isabelle Augenstein:
14:05-14:40,Keynote Speech Eduard Hovy 
14:40-15:15,Keynote Speech Hannah Rashkin: Challenges in measuring attribution in NLG models
15:15-15:50,Panel Discussion
16:00-17:30,Poster Session